{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n",
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "C:\\Users\\taro\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nfrom torch import nn\\nfrom torch.utils.data import DataLoader\\nfrom torchvision import datasets\\nfrom torchvision.transforms import ToTensor, Lambda, Compose\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "#import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "%matplotlib inline\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd: C:\\Users\\taro\\Documents\\Kaggle\\Recsys\n",
      "work: C:\\Users\\taro\\Documents\\Kaggle\\Recsys\\dressipi_recsys2022\n"
     ]
    }
   ],
   "source": [
    "# path取得\n",
    "cd_folder = os.getcwd()\n",
    "work_folder = cd_folder + \"\\\\dressipi_recsys2022\"\n",
    "print(\"cd: \" + cd_folder)\n",
    "print(\"work: \" + work_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ加工"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rawtable\n",
    "- candidate_items: lb, finalで購入されるitem候補\n",
    "- item_features：itemの特徴\n",
    "- test_final_sessions: test用session(最終用）\n",
    "- test_leaderboard_sessions: test用session(lb用）\n",
    "- train_purchases: train用sessionと購入item_id\n",
    "- train_sessions: train用session\n",
    "\n",
    "疎行列（加工）\n",
    "- train_sessions_cand_sparse: train用sessionのうち，購入item_idがcandidate_itemsに含まれる\n",
    "- train_purchases_not_cand_sparse: train用sessionのうち，購入item_idがcandidate_itemsに含まれない\n",
    "- test_leaderboard_sessions_sparse: test用session(lb用)\n",
    "- test_final_sessions_sparse: test用session(最終用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self, data_path = work_folder):\n",
    "        self.data_path = data_path\n",
    "    \n",
    "    # RawTable取得\n",
    "    def load_rawtable(self):\n",
    "        \n",
    "        ### rawdata\n",
    "        candidate_items = pd.read_csv( work_folder + \"\\\\\" + \"candidate_items.csv\")\n",
    "        item_features = pd.read_csv( work_folder + \"\\\\\" + \"item_features.csv\")\n",
    "        test_final_sessions = pd.read_csv( work_folder + \"\\\\\" + \"test_final_sessions.csv\")\n",
    "        test_leaderboard_sessions = pd.read_csv( work_folder + \"\\\\\" + \"test_leaderboard_sessions.csv\")\n",
    "        train_purchases = pd.read_csv( work_folder + \"\\\\\" + \"train_purchases.csv\")\n",
    "        train_sessions = pd.read_csv( work_folder + \"\\\\\" + \"train_sessions.csv\")\n",
    "        \n",
    "        return candidate_items, item_features, test_final_sessions, test_leaderboard_sessions, train_purchases, train_sessions\n",
    "    \n",
    "    # RawTable -> 疎行列\n",
    "    def load_sparsetable(self):\n",
    "        \n",
    "        candidate_items, item_features, test_final_sessions, test_leaderboard_sessions, train_purchases, train_sessions = self.load_rawtable()\n",
    "        \n",
    "        ### 疎行列作成\n",
    "        # train_dataのうち，購入itemがcandidate_itemに含まれるsessionを抽出\n",
    "        candidate_items_flg = candidate_items['item_id'].unique()\n",
    "        candidate_session_flg = train_purchases[train_purchases['item_id'].isin(candidate_items_flg)]['session_id'].values\n",
    "\n",
    "        # train_dataのうち購入itemがcandidate_itemに含まれるsession\n",
    "        train_purchases_cand = train_purchases[train_purchases['session_id'].isin(candidate_session_flg)]\n",
    "        train_sessions_cand  = train_sessions[train_sessions['session_id'].isin(candidate_session_flg)]\n",
    "\n",
    "        # train_dataのうち購入itemがcandidate_itemに含まれないsession\n",
    "        train_purchases_not_cand = train_purchases[~train_purchases['session_id'].isin(candidate_session_flg)]\n",
    "        train_sessions_not_cand  = train_sessions[~train_sessions['session_id'].isin(candidate_session_flg)]\n",
    "\n",
    "        # 疎行列のvalue作成\n",
    "        train_purchases_cand['count'] = 1\n",
    "        train_sessions_cand['count'] = 1\n",
    "        train_purchases_not_cand['count'] = 1\n",
    "        train_sessions_not_cand['count'] = 1\n",
    "        test_leaderboard_sessions['count'] = 1\n",
    "        test_final_sessions['count'] = 1\n",
    "        \n",
    "        index = 'session_id'\n",
    "        col = 'item_id'\n",
    "        val = 'count'\n",
    "        col_list = item_features['item_id'].unique()\n",
    "\n",
    "        train_sessions_cand_sparse = self.make_sparse_pivot(train_sessions_cand, index, col, val, col_list = col_list)\n",
    "        train_purchases_not_cand_sparse = self.make_sparse_pivot(train_purchases_not_cand, index, col, val, col_list = col_list)\n",
    "        test_leaderboard_sessions_sparse = self.make_sparse_pivot(test_leaderboard_sessions, index, col, val, col_list = col_list)\n",
    "        test_final_sessions_sparse = self.make_sparse_pivot(test_final_sessions, index, col, val, col_list = col_list)\n",
    "        \n",
    "        return train_sessions_cand_sparse, train_purchases_not_cand_sparse, test_leaderboard_sessions_sparse, test_final_sessions_sparse\n",
    "    \n",
    "    #参考URL: https://developers.microad.co.jp/entry/2019/05/10/180000\n",
    "    #疎行列のpivotテーブルを作成\n",
    "    def make_sparse_pivot(self, df, index= 'session_id', col = 'item_id', val = 'count', index_list = [], col_list = []): \n",
    "        if(len(index_list) == 0):\n",
    "            index_list = df[index].unique()\n",
    "        if(len(col_list) == 0):\n",
    "            col_list = df[col].unique()\n",
    "        index_categorical = pd.api.types.CategoricalDtype(categories=sorted(index_list), ordered=True)\n",
    "        col_categorical = pd.api.types.CategoricalDtype(categories=sorted(col_list), ordered=True)\n",
    "        row_num = df[index].astype(index_categorical).cat.codes\n",
    "        col_num = df[col].astype(col_categorical).cat.codes\n",
    "\n",
    "        sparse_matrix = csr_matrix((df[val], (row_num, col_num)), \\\n",
    "                               shape=(index_categorical.categories.size, \\\n",
    "                                      col_categorical.categories.size))\n",
    "        return sparse_matrix\n",
    "    \n",
    "    # validationデータ作成(作成中,load_sparsetableをtrain用とtest用に分割してtrain用をsplit_dataに流用する, sparseのindexはどうとる?, )\n",
    "    def split_data(self, df_session_sparse, test_size = 0.2, seed = 0):\n",
    "        X_train, X_test = train_test_split(df_session_sparse, test_size = test_size, random_state = seed)\n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データ読み込み\n",
    "warnings.simplefilter('ignore')\n",
    "data_loader = DataLoader(data_path = work_folder)\n",
    "# rawtable（testの購入候補，itemの特徴，test_final, test_lb, train購入item, train）\n",
    "candidate_items, item_features, test_final_sessions, test_leaderboard_sessions, train_purchases, train_sessions = data_loader.load_rawtable()\n",
    "# 疎行列(trainのうち，testの購入候補が含まれるsession, trainのうち，testの購入候補が含まれないsession, test_lb, test_final)\n",
    "train_sessions_cand_sparse, train_purchases_not_cand_sparse, test_leaderboard_sessions_sparse, test_final_sessions_sparse = data_loader.load_sparsetable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アルゴリズム実装（順次追加）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    \n",
    "    def KNN(self, df_train_sparse = train_sessions_cand_sparse, df_train_purchases = train_purchases, \\\n",
    "            item_features = item_features, \\\n",
    "            n_neighbors_fit = 10, df_test = test_leaderboard_sessions,  \\\n",
    "            df_test_sparse = test_leaderboard_sessions_sparse, n_neighbors_show = 10000):\n",
    "        \n",
    "        # KNNで訓練\n",
    "        #評価:cos類似度, bruteは総当り方式。\n",
    "        rec = NearestNeighbors(n_neighbors=n_neighbors_fit, algorithm= \"brute\", metric= \"cosine\")\n",
    "        rec_model = rec.fit(df_train_sparse)\n",
    "    \n",
    "        #予測：60分くらい\n",
    "        df_sub_lb = pd.DataFrame(columns=['session_id', 'item_id', 'rank'])\n",
    "        item_id = []\n",
    "        session_id = []\n",
    "        rank = []\n",
    "        count = 0\n",
    "        \n",
    "        unique_session_lb = df_test['session_id'].unique()                 #testのsession(unique)\n",
    "        unique_item_id_list = item_features['item_id'].unique()            #全item_id(unique)\n",
    "        df_train_purchases_ri = df_train_purchases.set_index('session_id') #trainの各sessionの購入item\n",
    "        session_id_list = df_train_purchases_ri.index                      #trainのsession(unique)\n",
    "        \n",
    "        # testの各session毎に類似度上位n_neighbors_showの購入item，距離を取得．距離で重みづけをしてスコアが高い100アイテムを取得\n",
    "        for i,s  in enumerate(unique_session_lb):\n",
    "            #for文のカウント,50000で終わり\n",
    "            if(count%1000 == 0):\n",
    "                print(count)\n",
    "            count += 1\n",
    "\n",
    "            #対象のlb sessionとの距離が近い、上位num_n_neighborsのtrain session の、indexと距離を取得\n",
    "            distance, indice = rec_model.kneighbors(df_test_sparse.getrow(i).toarray().flatten().reshape(1,-1),n_neighbors=n_neighbors_show)\n",
    "            df_distance = pd.DataFrame(np.array([df_train_purchases_ri.loc[session_id_list[indice].flatten()].item_id.values, 1 - distance[0]]).T, columns=['candidate_item_id', 'distance'])\n",
    "\n",
    "            #そのsessionに登場したアイテムを除く\n",
    "            session_item_list = df_test[df_test['session_id']==s].item_id.values\n",
    "            df_distance = df_distance[df_distance['candidate_item_id'].isin(session_item_list)==False]\n",
    "\n",
    "            #距離をもとにした重みで、アイテムごとの加重平均をとる->スコアが高い上位100アイテムを取得\n",
    "            prob_top100_item_id=df_distance.groupby('candidate_item_id').sum().sort_values('distance', ascending = 0).index.astype('int').values[0:100]\n",
    "\n",
    "            #結果を格納していく\n",
    "            item_id = np.concatenate([item_id,prob_top100_item_id])\n",
    "            temp = np.array([s for i in range(len(prob_top100_item_id))])\n",
    "            session_id = np.concatenate([session_id,temp])\n",
    "            temp = np.array([i + 1 for i in range(len(prob_top100_item_id))])\n",
    "            rank = np.concatenate([rank,temp])\n",
    "            \n",
    "            #提出用のデータを作成\n",
    "            df_sub_lb = pd.DataFrame(np.vstack([session_id, item_id, rank]).T, columns=['session_id', 'item_id', 'rank'])\n",
    "            df_sub_lb = df_sub_lb.astype('int')#intに明示的に変換しないと通らない\n",
    "            \n",
    "        return(df_sub_lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価指標（未チェック）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricCalculator:\n",
    "    \n",
    "    def calc(self, pred, y): #predはsession_id. item_id, rankの3列, yはsession_id, \n",
    "\n",
    "        mean_reciprocal_rank = self._calc_mean_reciprocal_rank()\n",
    "        return mean_reciprocal_rank\n",
    "    \n",
    "    #!!正しいかチェックできていない!!\n",
    "    def  _calc_mean_reciprocal_rank(self):\n",
    "        \n",
    "        Q = len(y)\n",
    "        point = 0\n",
    "        pred_session = pred[\"session_id\"].unique().tolist()\n",
    "        \n",
    "        #predとyのlengthチェック\n",
    "        if Q != len(pred_session):\n",
    "            print(\"Error: len pred is not equal to len y\")\n",
    "            break\n",
    "        #各sessionのpredにyのitem_idが含まれていた場合，predの1/rankを足し，最後にセッション数で除算\n",
    "        for session in range(pred_session):\n",
    "            pred_session = pred.loc[pred[\"session_id\"]==session]\n",
    "            y_session = y.loc[y[\"session_id\"]==session]\n",
    "            y_session_item = y_session[\"item_id\"].value\n",
    "            if y_session_item in pred_session[\"item_id\"].tolist():\n",
    "                point += 1.0 /(pred_session.loc[pred_session[\"item_id\"]==y_session_item][\"rank\"].value)\n",
    "            else:\n",
    "                continue\n",
    "        point = point / Q\n",
    "        \n",
    "        return point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.計算実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. データの読み込み（上で読み込み済）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_loader = DataLoader(data_path = work_folder)\n",
    "\n",
    "### rawtable（testの購入候補，itemの特徴，test_final, test_lb, train購入item, train）\n",
    "#candidate_items, item_features, test_final_sessions, test_leaderboard_sessions, train_purchases, train_sessions = data_loader.load_rawtable()\n",
    "\n",
    "### 疎行列(trainのうち，testの購入候補が含まれるsession, trainのうち，testの購入候補が含まれないsession, test_lb, test_final)\n",
    "#train_sessions_cand_sparse, train_purchases_not_cand_sparse, test_leaderboard_sessions_sparse, test_final_sessions_sparse = data_loader.load_sparsetable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. アルゴリズム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "recommender = Recommender()\n",
    "#df_train_sparse...trainの疎行列\n",
    "#df_train_purchases...trainの購入データ\n",
    "#item_features...(raw_table)\n",
    "#n_neighbors_fit...学習用の近傍点\n",
    "#df_test...testデータ\n",
    "#df_test_sparse...testの疎行列\n",
    "#n_neighbors_show...重みづけ計算\n",
    "result_KNN = recommender.KNN(df_train_sparse = train_sessions_cand_sparse, df_train_purchases = train_purchases, \\\n",
    "            item_features = item_features, \\\n",
    "            n_neighbors_fit = 10, df_test = test_leaderboard_sessions,  \\\n",
    "            df_test_sparse = test_leaderboard_sessions_sparse, n_neighbors_show = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>22676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>28067</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>13596</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>14550</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>4219</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999995</th>\n",
       "      <td>4439757</td>\n",
       "      <td>26433</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999996</th>\n",
       "      <td>4439757</td>\n",
       "      <td>648</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999997</th>\n",
       "      <td>4439757</td>\n",
       "      <td>17979</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999998</th>\n",
       "      <td>4439757</td>\n",
       "      <td>11891</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999999</th>\n",
       "      <td>4439757</td>\n",
       "      <td>11382</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         session_id  item_id  rank\n",
       "0                26    22676     1\n",
       "1                26    28067     2\n",
       "2                26    13596     3\n",
       "3                26    14550     4\n",
       "4                26     4219     5\n",
       "...             ...      ...   ...\n",
       "4999995     4439757    26433    96\n",
       "4999996     4439757      648    97\n",
       "4999997     4439757    17979    98\n",
       "4999998     4439757    11891    99\n",
       "4999999     4439757    11382   100\n",
       "\n",
       "[5000000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. 評価指標計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "実装中\n"
     ]
    }
   ],
   "source": [
    "pred = df_sub_lb\n",
    "y = df_sub_lb #trainを分割している場合ここはvalidationの正解データを入れる\n",
    "metric_calculator = MetricCalculator()\n",
    "metrics = metric_calculator.calc(pred, y)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. アウトプット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存\n",
    "savefile = result_KNN\n",
    "file_name = \"submit20220508_v1\"\n",
    "savefile.to_csv(cd_folder + \"\\\\\" + file_name + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下メモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 13198)\t1\n",
      "  (1, 216)\t1\n",
      "  (1, 1725)\t2\n",
      "  (1, 3553)\t1\n",
      "  (1, 13739)\t1\n",
      "  (1, 21891)\t1\n",
      "  (1, 22292)\t1\n",
      "  (1, 23209)\t1\n",
      "  (2, 8756)\t1\n",
      "  (2, 14743)\t1\n",
      "  (2, 17332)\t1\n",
      "  (2, 18989)\t1\n",
      "  (3, 1806)\t1\n",
      "  (3, 5205)\t1\n",
      "  (3, 13271)\t1\n",
      "  (3, 14530)\t1\n",
      "  (3, 16667)\t1\n",
      "  (3, 17906)\t1\n",
      "  (4, 8798)\t2\n",
      "  (4, 13463)\t1\n",
      "  (4, 14028)\t2\n",
      "  (4, 16621)\t3\n"
     ]
    }
   ],
   "source": [
    "print(train_sessions_cand_sparse[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<450153x23691 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1770447 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions_cand_sparse\n",
    "#train_purchases_not_cand_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x23691 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 189283 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_leaderboard_sessions_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sparse行列のチェック\n",
    "# https://note.nkmk.me/python-scipy-sparse-get-element-row-column/\n",
    "\n",
    "csr_matrix(train_sessions_cand_sparse).getrow(2).toarray()\n",
    "\n",
    "csr_matrix(train_sessions_cand_sparse).getrow(2).toarray().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions_cand_sparse.getrow(0).toarray().flatten().reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id_list = test_leaderboard_sessions[\"session_id\"].unique().tolist()\n",
    "\n",
    "#trainと同列のデータ作成用\n",
    "df_base_col = pd.DataFrame(np.array(col_list), columns = [\"item_id\"]).astype(\"category\")\n",
    "df_base_col[\"session_id\"] = 99999999\n",
    "df_base_col[\"count\"] = 1\n",
    "df_base_col = df_base_col[[\"session_id\",\"item_id\",\"count\"]]\n",
    "\n",
    "batch = 100\n",
    "n_rep = len(test_id_list) // batch\n",
    "\n",
    "#batchサイズ毎の処理\n",
    "#for i_rep in range(n_rep):\n",
    "for i_rep in range(2):\n",
    "    \n",
    "    # test_session_id取得（batchサイズ毎）\n",
    "    if(i_rep == n_rep - 1):\n",
    "        test_id_list_batch = test_id_list[ i_rep * batch : len(test_id_list) ]\n",
    "    else:\n",
    "        test_id_list_batch = test_id_list[ i_rep * batch : i_rep * batch + batch ]\n",
    "        \n",
    "    # \n",
    "    a = test_leaderboard_sessions.loc[test_leaderboard_sessions[\"session_id\"].isin(test_id_list_batch)][[\"session_id\",\"item_id\",\"count\"]]\n",
    "    a[\"item_id\"] = a[\"item_id\"].astype(\"category\")\n",
    "    a = pd.concat([a,df_base_col])\n",
    "    a = a.pivot_table(index = \"session_id\", columns = \"item_id\", values = \"count\", fill_value = 0)\n",
    "    #a = a.loc[~a[\"session_id\"]==99999999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sparseのindexはどのように取得する? -> splitで使用したい ->最終的には交差検証\n",
    "- item情報どう入れる?\n",
    "- candではないsessionは使用できる?\n",
    "- KNN以外に良い方法がある?\n",
    "- KNN高速化できる?\n",
    "- Todo validationデータ作成して，KNNのパラメータを変えてみて精度の検証（上の疑問点は解決する必要あり）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
